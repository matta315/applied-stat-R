---
title: "Week 3 - Homework"
author: "STAT 420, Summer 2020, D. Unger"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---
***

## Exercise 1 (Using `lm` for Inference)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Fit the following simple linear regression model in `R`. Use heart weight as the response and body weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `cat_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

**Answer**

```{r, echo=FALSE}
library(MASS)

```

```{r}
cat_model = lm(Hwt ~ Bwt, data = cats)
summary(cat_model)
```

 \[
H_0: \beta_1 = 0 \quad \text{vs} \quad H_1: \beta_1 \neq 0
\]

**Explain**

- Under $H_0$ there is not a significant linear relationship between body weight and heart weight
- Under $H_1$ there is a significant linear relationship between body weight and heart weight
- The value of the test statistic for $\beta_1$ is `r summary(cat_model)$coefficients[2,3]`
- The p-value of the test for $\beta_1$ is `r summary(cat_model)$coefficients[2,4]`
- A statistical decision at $\alpha = 0.05$
- With this low p-value compared to our $\alpha = 0.05$, we would reject this null hypothesis. Therefore the conclusion is there IS a significant linear relationship between cat's body weight and heart weight 

**(b)** Calculate a 95% confidence interval for $\beta_1$. Give an interpretation of the interval in the context of the problem.
**Answer**

```{r, message=FALSE, warning=FALSE}
CI_beta1 = confint(cat_model, parm = "Bwt" ,level = 0.95)
CI_beta1
```

95% confident that for an increase of body weight (in kg), the average increase for heart weight (in kg) is between `r CI_beta1[1]` and `r CI_beta1[2]` kg, which is the interval for $\beta_1$

**(c)** Calculate a 90% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.
**Answer**

```{r, message=FALSE, warning=FALSE}
CI_beta0 = confint(cat_model, parm = "(Intercept)" ,level = 0.90)
CI_beta0
```

We are 90% confident that the average heart weight of cat when there is 0kg body weight is between `r CI_beta0[1]` and  `r CI_beta0[2]` kg. However we shouldn't be strongly believe in it since we are certain that the heart weight shouldn't be negative no matter what

**(d)** Use a 90% confidence interval to estimate the mean heart weight for body weights of 2.1 and 2.8 kilograms. Which of the two intervals is wider? Why?
**Answer**

```{r}
# Body weight at 2.1
width1 = predict(cat_model, newdata = data.frame(Bwt = 2.1),
        interval = c("confidence"), level = 0.90
        )[1,3] - predict(cat_model, newdata = data.frame(Bwt = 2.1),
        interval = c("confidence"), level = 0.90
        )[1,2]

width1
```
```{r}
# Body weight at 2.8
width2 = predict(cat_model, newdata = data.frame(Bwt = 2.6),
        interval = c("confidence"), level = 0.90
        )[1,3] - predict(cat_model, newdata = data.frame(Bwt = 2.6),
        interval = c("confidence"), level = 0.90
        )[1,2]

width2

```

We also have

```{r}
mean(cats$Bwt)
```

As we know the further you move away from the mean value, the higher range of your confidencce interval. Due to information above we can notice that 2 intervals of 2.1 having the width as `r width1` than width of 2.6's bodyweight interval of `r width2` because 2.1 is further from the mean: `mean(cats$Bwt)`. 

**(e)** Use a 90% prediction interval to predict the heart weight for body weights of 2.8 and 4.2 kilograms.
**Answer**

```{r}
new_bwt = data.frame(Bwt = c(2.8,4.2))

predict(cat_model, newdata = new_bwt,
        interval = "prediction",level = 0.90)
```


**(f)** Create a scatterplot of the data. Add the regression line, 95% confidence bands, and 95% prediction bands.
**Answer**
```{r}
grid = seq(min(cats$Bwt), max(cats$Bwt), by = 0.01)
cats_ci_band = predict(cat_model, 
                       newdata = data.frame(Bwt = grid), 
                       interval = "confidence", level = 0.99)
cats_pi_band = predict(cat_model, 
                       newdata = data.frame(Bwt = grid), 
                       interval = "prediction", level = 0.99) 

plot(Hwt ~ Bwt, data = cats,
     xlab = "Bodyweight (in kg)",
     ylab = "Heartweight (in kg)",
     main = "Heartweight vs Bodyweight",
     pch  = 20,
     cex  = 2,
     col  = "grey",
     ylim = c(min(cats_pi_band), max(cats_pi_band)))

abline(cat_model, lwd = 3, col = "darkorange")

lines(grid, cats_ci_band[,"lwr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(grid, cats_ci_band[,"upr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(grid, cats_pi_band[,"lwr"], col = "dodgerblue", lwd = 3, lty = 3)
lines(grid, cats_pi_band[,"upr"], col = "dodgerblue", lwd = 3, lty = 3)
points(mean(cats$Bwt), mean(cats$Hwt), pch = "+", cex = 3)
```


**(g)** Use a $t$ test to test:

- $H_0: \beta_1 = 4$
- $H_1: \beta_1 \neq 4$

Report the following:

- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

**Answer**
```{r}
beta_hat_1_sd = summary(cat_model)$coefficients["Bwt","Std. Error"]
beta_hat_1_EST = summary(cat_model)$coefficients["Bwt","Estimate"]

t_test_cal = function(EST = 1, HVP = 0, SE = 1){
  (EST - HVP)/SE
}

t_4 = t_test_cal(EST = beta_hat_1_EST, HVP = 4, SE = beta_hat_1_sd )

P_4 = 2 * pt(-abs(t_4), df = nrow(cats) - 2)
```


- The value of the test statistic is `r t_4`
- The p-value of the test`r P_4`
- With $\alpha = 0.05$  and p-value as `r P_4` we would fail to reject $H_0$, so we sat there is not a significant linear relationship between Cat body's weight and heartweight

***

## Exercise 2 (More `lm` for Inference)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will re-perform the data cleaning done in the previous homework.

```{r}
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

**(a)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and wind speed as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_wind_model`. Use a $t$ test to test the significance of the regression. Report the following:

```{r}
ozone_wind_model = lm(ozone ~ wind, data = Ozone)
```

```{r}
summary(ozone_wind_model)
```

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

**Answer**

 \[
H_0: \beta_1 = 0 \quad \text{vs} \quad H_1: \beta_1 \neq 0
\]

**Explain**

- Under $H_0$ there is not a significant linear relationship between body weight and heart weight

- Under $H_1$ there is a significant linear relationship between body weight and heart weight

- The value of the test statistic for $\beta_1$ is `r summary(ozone_wind_model)$coefficients[2,3]`

- The p-value of the test for $\beta_1$ is `r summary(ozone_wind_model)$coefficients[2,4]`

- A statistical decision at $\alpha = 0.01$

With this low p-value compared to our $\alpha = 0.01$, we would fail to reject this null hypothesis. Therefore the conclusion is there is not a significant linear relationship between wind and ozone 

**(b)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and temperature as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_temp_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

**Answer**
```{r}
ozone_temp_model = lm(ozone ~ temp, data = Ozone)
```

```{r}
summary(ozone_temp_model)
```

 \[
H_0: \beta_1 = 0 \quad \text{vs} \quad H_1: \beta_1 \neq 0
\]
- Under $H_0$ there is not a significant linear relationship between body weight and heart weight
- Under $H_1$ there is a significant linear relationship between body weight and heart weight
- The value of the test statistic for $\beta_1$ is `r summary(ozone_temp_model)$coefficients[2,3]`
- The p-value of the test for $\beta_1$ is `r summary(ozone_temp_model)$coefficients[2,4]`
- A statistical decision at $\alpha = 0.01$
- With this low p-value compared to our $\alpha = 0.01$, we would **reject** this null hypothesis. Therefore the conclusion is there **is** a significant linear relationship between wind and tempature

***

## Exercise 3 (Simulating Sampling Distributions)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = -5$
- $\beta_1 = 3.25$
- $\sigma^2 = 16$

We will use samples of size $n = 50$.

**(a)** Simulate this model $2000$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_0$ and $\hat{\beta}_1$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 18760613
set.seed(birthday)
n = 50
x = seq(0, 10, length = n)

beta_0 = -5
beta_1 = 3.25
sigma = 4
Sxx = sum((x- mean(x))^2)
```


```{r}
#simulate data function

sim_slr = function(x, beta_0 = 5, beta_1 = -3, sigma = 1) {
  n = length(x)
  epsilon = rnorm(n, mean = 0, sd = sigma)
  y = beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response = y)
}
```

```{r}
rep_time = 2000
beta_0_hats = rep(0,rep_time)
beta_1_hats = rep(0,rep_time)

for (i in 1:rep_time) {
  sim_data = sim_slr(x, beta_0 = -5, beta_1 = 3.25, sigma = 4)
  model = lm(response ~ predictor, data = sim_data)
  beta_0_hats[i] = coef(model)[1]
  beta_1_hats[i] = coef(model)[2]
}
```

**(b)** Create a table that summarizes the results of the simulations. The table should have two columns, one for $\hat{\beta}_0$ and one for $\hat{\beta}_1$. The table should have four rows:

- A row for the true expected value given the known values of $x$
- A row for the mean of the simulated values
- A row for the true standard deviation given the known values of $x$
- A row for the standard deviation of the simulated values

**Answer**

```{r}
#true Sd

sd_beta_1_hat = sqrt(sigma ^ 2 / Sxx)

sd_beta_0_hat = sqrt(sigma ^ 2 * (1 / n + mean(x) ^ 2 / Sxx))
```


```{r}
#create dataframe
result = data.frame(
  Beta = c("beta_hat_0","beta_hat_1"),
  ExpectedVal = c(beta_0,beta_1),
  SimulatedVal = c(mean(beta_0_hats),mean(beta_1_hats)),
  SD = c(sd_beta_0_hat,sd_beta_1_hat),
  SE = c(sd(beta_0_hats),sd(beta_1_hats))
)

knitr::kable(t(result))
```


**(c)** Plot two histograms side-by-side:

- A histogram of your simulated values for $\hat{\beta}_0$. Add the normal curve for the true sampling distribution of $\hat{\beta}_0$.
- A histogram of your simulated values for $\hat{\beta}_1$. Add the normal curve for the true sampling distribution of $\hat{\beta}_1$.


```{r,echo=FALSE}
par(mfrow = c(1,2))

hist(beta_1_hats, prob = TRUE, breaks = 25, 
     xlab = expression(hat(beta)[1]), 
     main = "",
     border = "dodgerblue")

# Adding curve for normal distribution
curve(dnorm(x, mean = beta_1, sd = sd_beta_1_hat),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_0_hats, prob = TRUE, breaks = 25, 
     xlab = expression(hat(beta)[0]), 
     main = "",
     border = "dodgerblue")

# Adding curve for normal distribution
curve(dnorm(x, mean = beta_0, sd = sd_beta_0_hat),
      col = "darkorange", add = TRUE, lwd = 3)
```

***

## Exercise 4 (Simulating Confidence Intervals)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 5$
- $\beta_1 = 2$
- $\sigma^2 = 9$

We will use samples of size $n = 25$.

Our goal here is to use simulation to verify that the confidence intervals really do have their stated confidence level. Do **not** use the `confint()` function for this entire exercise.

**(a)** Simulate this model $2500$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_1$ and $s_e$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 18760613
set.seed(birthday)
n = 25
x = seq(0, 2.5, length = n)

beta_0 = 5
beta_1 = 2
sigma = 9/2
Sxx = sum((x- mean(x))^2)
```

```{r}
sim_slr = function(x, beta_0 = 5, beta_1 = -3, sigma = 1) {
  n = length(x)
  epsilon = rnorm(n, mean = 0, sd = sigma)
  y = beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response = y)
}
```

```{r}
rep_time = 2500
s_e = rep(0,rep_time)
beta_1_hats = rep(0,rep_time)

for (i in 1:rep_time) {
  sim_data = sim_slr(x, beta_0 = beta_0, beta_1 = beta_1, sigma = 3)
  model = lm(response ~ predictor, data = sim_data)
  s_e[i] = summary(model)$sigma
  beta_1_hats[i] = coef(model)[2]
}
```

**(b)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 95% confidence interval. Store the lower limits in a vector `lower_95` and the upper limits in a vector `upper_95`. Some hints:

- You will need to use `qt()` to calculate the critical value, which will be the same for each interval.
- Remember that `x` is fixed, so $S_{xx}$ will be the same for each interval.
- You could, but do not need to write a `for` loop. Remember vectorized operations.

**Answer**

```{r}
alpha = (1 - 0.95)/2
t_crit_95 = qt(1 - alpha, df = n - 2)

lower_95 = beta_1_hats - t_crit_95 * s_e/sqrt(Sxx)
upper_95 = beta_1_hats + t_crit_95 * s_e/sqrt(Sxx)

```


**(c)** What proportion of these intervals contains the true value of $\beta_1$?

**Answer**
```{r}
mean(lower_95 < beta_1 & beta_1 < upper_95)
```

**(d)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.05$?

**Answer**

```{r}
1 - mean(lower_95 < 0 & 0 < upper_95)
```

**(e)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 99% confidence interval. Store the lower limits in a vector `lower_99` and the upper limits in a vector `upper_99`.

**Answer**
```{r}
alpha = (1 - 0.99)/2
t_crit_99 = qt(1 - alpha, df = n - 2)

lower_99 = beta_1_hats - t_crit_99 * s_e/sqrt(Sxx)
upper_99 = beta_1_hats + t_crit_99 * s_e/sqrt(Sxx)
```



**(f)** What proportion of these intervals contains the true value of $\beta_1$?

**Answer**

```{r}
mean(lower_99 < beta_1 & beta_1 < upper_99)
```

**(g)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.01$?

**Answer**
```{r}
1 - mean(lower_99 < 0 & 0 < upper_99)
```

***

## Exercise 5 (Prediction Intervals "without" `predict`)

Write a function named `calc_pred_int` that performs calculates prediction intervals:

$$
\hat{y}(x) \pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{1 + \frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}.
$$

for the linear model

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$

**(a)** Write this function. You may use the `predict()` function, but you may **not** supply a value for the `level` argument of `predict()`. (You can certainly use `predict()` any way you would like in order to check your work.)

The function should take three inputs:

- `model`, a model object that is the result of fitting the SLR model with `lm()`
- `newdata`, a data frame with a single observation (row)
    - This data frame will need to have a variable (column) with the same name as the data used to fit `model`.
- `level`, the level (0.90, 0.95, etc) for the interval with a default value of `0.95`

The function should return a named vector with three elements:

- `estimate`, the midpoint of the interval
- `lower`, the lower bound of the interval
- `upper`, the upper bound of the interval

```{r}
calc_pred_int = function(model = lm(y ~ x), newdata, level = 0.95){
  x = model$model[,2]
  mean_x = mean(model$model[,2])
  n = nrow(model$model)
  s_e = summary(model)$sigma
  crit = qt(1-(1 - level)/2, df = n - 2)
  Sxx = sum((x - mean(x))^2)
  est = predict(model, newdata = newdata)
  
  lower = est - crit * s_e * sqrt(1 - 1/n + (newdata - mean_x)^2/Sxx)
  higher = est + crit * s_e * sqrt(1 - 1/n + (newdata - mean_x)^2/Sxx)
  
  c(est, lower, higher)
}
```


**(b)** After writing the function, run this code:

```{r, eval = FALSE}
newcat_1 = data.frame(Bwt = 4.0)
calc_pred_int(cat_model, newcat_1)
```

**(c)** After writing the function, run this code:

```{r, eval = FALSE}
newcat_2 = data.frame(Bwt = 3.3)
calc_pred_int(cat_model, newcat_2, level = 0.90)
```



